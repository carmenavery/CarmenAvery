---
title: "Assignments"
output:
  html_document:
    toc: yes
    toc_float: yes
    collapsed: no
    number_sections: no
    toc_depth: 1
  pdf_document:
    toc: no
    toc_depth: '1'
---


This page will contain all the assignments you submit for the class.



### Instructions for all assignments

I want you to submit your assignment as a PDF, so I can keep a record of what the code looked like that day. I also want you to include your answers on your personal GitHub website. This will be good practice for editing your website and it will help you produce something you can keep after the class is over.

1. Download the Assignment1.Rmd file from Canvas. You can use this as a template for writing your answers. It's the same as what you can see on my website in the Assignments tab. Once we're done with this I'll edit the text on the website to include the solutions.

2. On RStudio, open a new R script in RStudio (File > New File > R Script). This is where you can test out your R code. You'll write your R commands and draw plots here.

3. Once you have finalized your code, copy and paste your results into this template (Assignment 1.Rmd). For example, if you produced a plot as the solution to one of the problems, you can copy and paste the R code in R markdown by using the ` ``{r} ``` ` command. Answer the questions in full sentences and Save.

4. Produce a PDF file with your answers. To do this, knit to PDF (use Knit button at the top of RStudio), locate the PDF file in your docs folder (it's in the same folder as the Rproj), and submit that on on Canvas in Assignment 1.

5. Build Website, go to GitHub desktop, commit and push. Now your solutions should be on your website as well.






# Assignment 1

**Collaborators: Rachael Villari, Elizabeth Stoner, and Halle Wasser**

This assignment is due on Canvas on Monday 9/20 before class, at 10:15 am. Include the name of anyone with whom you collaborated at the top of the assignment.


### Problem 1 

Install the datasets package on the console below using `install.packages("datasets")`. Now load the library.

```{r}
library(knitr)
library(datasets)
```

Load the USArrests dataset and rename it `dat`. Note that this dataset comes with R, in the package datasets, so there's no need to load data from your computer. Why is it useful to rename the dataset?

```{r}
dat <- USArrests
```

**Answer: Renaming the dataset provides a short-hand way of calling it as I code
          instead of trying to remember the name of the dataset.**

### Problem 2

Use this command to make the state names into a new variable called State. 

```{r, eval=FALSE}
dat$state <- tolower(rownames(USArrests))
```

This dataset has the state names as row names, so we just want to make them into a new variable. We also make them all lower case, because that will help us draw a map later - the map function requires the states to be lower case.


List the variables contained in the dataset `USArrests`.

```{r}
names(dat)
```

**Answer: The variables are Murder, Assault, UrbanPop, Rape, and state.**

### Problem 3 

What type of variable (from the DVB chapter) is `Murder`? 

**Answer: It is a quantitative/continuous variable.**

What R Type of variable is it?
```{r}
typeof("Murder")
```

**Answer: It is a character data type.**

### Problem 4

What information is contained in this dataset, in general? What do the numbers mean? 
```{r}
?USArrests
```

**Answer: This dataset contains 50 observations/instances on 5 variables: Murder 
          (murder arrests  per 100,000), Assault (assault arrests per 100,000), 
          UrbanPop (percent urban population), and Rape (rape arrests per 100,000), and state.**

### Problem 5

Draw a histogram of `Murder` with proper labels and title.

```{r}
hist(dat$Murder, main="Histogram of Murder Arrests", xlab="Murder Arrests \n 
     per 100,000", ylab="Frequency")
```

### Problem 6

Please summarize `Murder` quantitatively. What are its mean and median? What is the difference between mean and median? What is a quartile, and why do you think R gives you the 1st Qu. and 3rd Qu.?

```{r}
summary(dat$Murder)
```

**Answer: The mean number of murder arrests is 7.7888 per 100,000, and the median
          is 7.25 murder arrests per 100,000. According to HS, the mean and median
          are both measures of central tendency. However, the mean is described to
          be the value that each observation would receive if the values were redistributed
          among the dataset, and the mean on a histogram is the "balancing point". 
          The median is the value that is at the center if all the values were put 
          in an ordered list. According to HS, the interquartile range (IQR) is a measure 
          of spread, and the "quartiles...are the three values which divide the
          distribution into even fourths. The IQR is obtained by subtracting the 
          1st quartile from the 3rd quartile which is why R gives us both these
          values.**

### Problem 7

Repeat the same steps you followed for `Murder`, for the variables `Assault` and `Rape`. Now plot all three histograms together. You can do this by using the command `par(mfrow=c(3,1))` and then plotting each of the three. 

```{r, echo = TRUE, fig.width = 5, fig.height = 8}
summary(dat$Assault)
summary(dat$Rape)
```

**Answer: The mean number of assault arrests is 170.8 per 100,000, and the median
          is 159 assault arrests per 100,000. 
          The mean number of rape arrests is 21.23 per 100,000, and the median
          is 20.1 rape arrests per 100,000.**

What does the command par do, in your own words (you can look this up by asking R `?par`)?

**Answer: There are a lot of graphical features in R. Par() allows us to search and set them.**

```{r, echo = TRUE, fig.width = 5, fig.height = 8}
par(mfrow=c(3,1))
hist(dat$Murder, main="Histogram of Murder Arrests", xlab="Murder Arrests \n 
     per 100,000", ylab="Frequency")
hist(dat$Assault, main="Histogram of Assault Arrests", xlab="Assault Arrests \n 
     per 100,000", ylab="Frequency")
hist(dat$Rape, main="Histogram of Rape Arrests", xlab="Rape Arrests \n 
     per 100,000", ylab="Frequency")
```

What can you learn from plotting the histograms together?

**Answer: Murder and rape arrests are right skewed, which is good. What I thought
          was interesting were how the assault arrests are pretty normally distributed.
          Additionally, I see the average assault arrests per 100,000 were 
          much greater than the average murder or rape arrests per 100,000 which 
          makes sense.**
  
### Problem 8

In the console below (not in text), type `install.packages("maps")` and press Enter, and then type `install.packages("ggplot2")` and press Enter. This will install the packages so you can load the libraries.

Run this code:

```{r, eval = FALSE, fig.width = 7.5, fig.height = 4}
library('maps') # load maps library into R session
library('ggplot2') # load ggplot2 into R session

ggplot(dat, aes(map_id=state, fill=Murder)) + # call ggplot function, use dat 
                                              # dataframe, and set up plot aesthetics
  geom_map(map=map_data("state")) + # create map of all states
  expand_limits(x=map_data("state")$long, y=map_data("state")$lat) # expand plot limits
```

What does this code do? Explain what each line is doing.

**Answer: This code is creating a heat map of the murder arrests per 100,000 in 
          each state. See comments for a description of each line of code.**

$$\\[2in]$$




# Assignment 2 

### Problem 1
```{r}
#load library
library(tidyverse)
library(ggplot2)
# Read the data
dat <- read.csv(file = 'dat.nsduh.small.1.csv')
```

What are the dimensions of the dataset? 
```{r}
nrow(dat)
ncol(dat)
names(dat)
```

**Answer: This dataset contains 171 rows/observations and 7 columns.**

### Problem 2

What is this dataset about? Who collected the data, what kind of sample is it, and what was the purpose of generating the data?

**Answer: This dataset is a sample  of the 2019 National Survey of Drug Use and Health (NSDUH). According to their website (https://nsduhweb.rti.org/respweb/about_nsduh.html), NSDUH is sponsored by the Substance Abuse and Mental Health Services Administration (SAMHSA) and the US Dept. of Health and Human Services, and it serves as a sample to inform policymakers of tobacco, alcohol, and drug use in the US population as well as other health related concerns. Their website also states that they survey approximately 70,000 people each year ages 12 and up. The variables of the provided dataset are as follows: 'mjage', 'cigage', 'iralcage', 'age2', 'sexatract', 'speakengl', and 'irsex'. 'Mjage', 'cigage', and 'iralcage' refer to the age at which participants first started using marijuana or hashish, cigarrettes, and alcohol, respectively. The 'age2' variable is the age of the respondent at the end of the questionnaire. 'Sexatract', 'speakengl', and 'irsex' are the respondent's sexual attraction/orientation, English speaking level, and gender.**

### Problem 3: Age and gender

What is the age distribution of the sample like?

```{r}
hist(dat$age2, xlab = 'Age Category', main = 'Histogram of Age')
summary(dat$age2)
dat %>% group_by(age2) %>% count()
```
**Answer: It appears to be left skewed, and it looks like there's a peak at 15 which the codebook describes as being 35-49 years old. This means that the majority of the people within this dataset are not teenagers**

Do you think this age distribution representative of the US population? Why or why  not?

**Answer: No, because this is a very small sample size (171 people), and it wouldn't be representative of the millions of people that live in the US. Additionally, the NSDUH only surveys people aged 12 and older meaning children under 12 would be excluded from the age distribution. Also there are not many teens or people in their early twenties represented by this dataset.**

Is the sample balanced in terms of gender? If not, are there more females or males?

```{r}
hist(dat$irsex, xlab = 'Gender (1 = Male, 2 = Female)', main = 'Distribution of Gender')
```

**Answer: No, the sample is not balanced in terms of gender. There are more males (91 total) than females (80 total).**

```{r}
tab.agesex <- table(dat$irsex, dat$age2)
barplot(tab.agesex,
        main = "Stacked barchart",
        xlab = "Age category", ylab = "Frequency",
        legend.text = rownames(tab.agesex),
        beside = FALSE) # Stacked bars (default)
```

**Answer: It's difficult to be certain by just looking at this bar plot, but visually it appears as though this dataset contains males who are older and females who are younger. For example, the age categories of 14, 16, and 17 look to have more men than women while categories 8, 9, 10, 11, and 12 seem to have more females than males. This might also be because there are more males than females in this dataset. However, it is clear that the main age categories in this dataset are 13-17.**

### Problem 4: Substance use

For which of the three substances included in the dataset (marijuana, alcohol, and cigarettes) do individuals tend to use the substance earlier?

```{r}
summary(dat$mjage)
summary(dat$cigage)
summary(dat$iralcage)
hist(dat$mjage, xlab = 'Age Category of First Marijuana Usage', main = 'Distribution of First Marijuana Usage Age')
hist(dat$cigage, xlab = 'Age Category of First Cigarrete Usage', main = 'Distribution of First Cigarrete Usage Age')
hist(dat$iralcage,  xlab = 'Age Category of First Alcohol Usage', main = 'Distribution of First Alcohol Usage Age')
```

**Answer: From the summary statistics and the histograms, it looks like people tend to use alcohol earlier than cigarettes or marijuana, mainly around the age category of 15 (35-49 years old).**

### Question 5

What does the distribution of sexual attraction look like? Is this what you expected?

What is the distribution of sexual attraction by gender? 

```{r}
ggplot(dat, aes(x=sexatract)) + 
  geom_histogram() + 
  xlab('Sexual Attraction Category') +
  ggtitle('Distribution of Sexual Attraction \n Including Bad Data Entries')
```

**Answer: This histogram includes the entries where people chose not to answer or the entries were blank, answered, or bad data for whatever reason. I expected to see these types of entries in this dataset.**

```{r}
ggplot(dat, aes(x=sexatract)) + 
  geom_histogram() + 
  xlim(0, 6) + 
  xlab('Sexual Attraction Category') +
  ggtitle('Distribution of Sexual Attraction \n Excluding Bad Data Entries')
```

**Answer: I generated this histogram to isolate the sexual attraction categories by excluding the bad data entries. This clearly shows the majority of people in this dataset are only attracted to the opposite sex which is what I expected since heterosexuality is the majority sexual orientation in society. It is followed by mostly opposite sex attraction and attraction to both sexes. I would say none of this is a surprise given my sense of sexual orientations within the US.**

```{r}
tab.attsex <- table(dat$irsex, dat$sexatract)
barplot(tab.attsex,
        main = "Sexual Attraction Across Genders",
        xlab = 'Sexual Attraction Category',
        ylab = 'Frequency',
        legend.text = rownames(tab.attsex),
        beside = FALSE)
```

**Answer: This plot shows that the males mostly comprised the opposite sex attraction category as well as the strictly same sex attraction and unsure sexual attraction categories. They were also the majority sex that skipped this question altogether. It's interesting that females were the majority in the mostly opposite sex attraction and the both sex attraction categories.**


###Problem 6: English speaking

What does the distribution of English speaking look like in the sample? Is this what you might expect for a random sample of the US population?

Are there more English speaker females or males?

```{r}
hist(dat$speakengl,
     xlab = 'English Speaking Level',
     main = 'Distribution of English Speaking Levels')
```

**Answer: The vast majority of people in this survey speak English very well followed by people who speak it well and those who don't speak it at all. I would expect this from a random sample of the US population since the dominant language is English although we are a melting pot country with a variety of people groups.**


```{r}
tab.englsex <- table(dat$irsex, dat$speakengl)
barplot(tab.englsex,
        main = "English Speaking Level Across Genders",
        xlab = 'English Speaking Level',
        ylab = 'Frequency',
        legend.text = rownames(tab.englsex),
        beside = FALSE)
dat %>% group_by(irsex, speakengl) %>% count()
```

**Answer: From these statistics and plot, it appears as though there are more male English speakers in this dataset, but one has to remember there are more males in the dataset overall. Additionally, it's interesting that there are no males who rated themselves as not speaking English at all.**


# Exam 1

Load the data into an R data frame.
```{r}
df <- read.csv("fatal-police-shootings-data.csv")
library(ggplot2)
library(tidyverse)
library(tidyr)
```


### Problem 1 (10 points)

a. Describe the dataset. This is the source: https://github.com/washingtonpost/data-police-shootings . Write two sentences (max.) about this.

__Answer: According to the codebook, this dataset consists of fatal shootings (observations) of civilians by police officers on duty since 2015. The dataset includes information (variables/columns) of the victims such as age, race, gender, threat level, etc.__

b. How many observations are there in the data frame?
```{r}
nrow(df)
```

__Answer: There are 6594 observations in this dataset.__

c. Look at the names of the variables in the data frame. Describe what "body_camera", "flee", and "armed" represent, according to the codebook. Again, only write one sentence (max) per variable.
```{r}
names(df)
```

__"Body_camera" indicates 'true' or 'false' if the officer wore a body camera and recorded a portion of the shooting. "Flee" is if the victim was fleeing by foot, car, or not fleeing, and "armed" tells if the victim's arm is determined or unknown or if they were not armed.__

d. What are three weapons that you are surprised to find in the "armed" variable? Make a table of the values in "armed" to see the options.
```{r}
table(df$armed)
```

__Answer: It's kind of funny to see an air conditioner, binoculars, and microphone as weapons.__

### Problem 2 (10 points)

a. Describe the age distribution of the sample. Is this what you would expect to see?
```{r}
hist(df$age, main = 'Distribution of Age', xlab = 'Age')
```

__Answer: The age distribution is right skewed and unimodal with the peak being at around early to mid-30s. I would somewhat expect to see this because it's probable that police don't view older adults as threats typically, but I'm not exactly sure of this.__

b. To understand the center of the age distribution, would you use a mean or a median, and why? Find the one you picked.
```{r}
summary(df$age)
```

__I would use the median (35 years old) because the age distribution is right skewed, so if the mean were to be used, it wouldn't give an accurate representation of centrality.__

c. Describe the gender distribution of the sample. Do you find this surprising?
```{r}
barplot(table(df$gender), main = "Distribution of Gender")
df %>% group_by(gender) %>% count()
```

__Answer: The gender distribution is very unbalanced with males making up the majority (6298/6594 = 95.5%) of the dataset. This surprises me because although I expected more males in the dataset, I didn't think they would make up about 96% of the dataset. It looks like the gender column also contains some null values.__


### Problem 3 (10 points)

a. How many police officers had a body camera, according to news reports? What proportion is this of all the incidents in the data? Are you surprised that it is so high or low?

```{r}
barplot(table(df$body_camera), main = 'Body Camera on Officer')
df %>% group_by(body_camera) %>% count() %>% mutate(proportion = n/nrow(df))
```

__Answer: Very few, only 13.8%, officers had a body camera. This doesn't surprise me because when police are on duty, I would imagine they don't wear cameras on their person.__

b. In  how many of the incidents was the victim fleeing? What proportion is this of the total number of incidents in the data? Is this what you would expect?
```{r}
barplot(table(df$flee), main = 'Distribution of Flee Methods', xlab = 'Flee Method')
df %>% group_by(flee) %>% count() %>% mutate(proportion = n/nrow(df))
```

__Answer: Victims fleeing by car or foot make up 28.9% of the dataset. This number excludes null or 'other' values. This is expected because I imagine most people don't flee from police.__



### Problem 4 (10 points) 

a. Describe the relationship between the variables "body camera" and "flee" using a stacked barplot. What can you conclude from this relationship? 

*Hint 1: The categories along the x-axis are the options for "flee", each bar contains information about whether the police officer had a body camera (vertically), and the height along the y-axis shows the frequency of that category).*

*Hint 2: Also, if you are unsure about the syntax for barplot, run ?barplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.*


```{r}
tab.camflee <- table(df$body_camera, df$flee)
barplot(tab.camflee,
        main = "Relationship Between Body Cams and Fleeing Victims",
        xlab = 'Type of Fleeing',
        ylab = 'Frequency',
        legend.text = rownames(tab.camflee),
        beside = FALSE)
df %>% group_by(body_camera, flee) %>% count()
```

__Answer: It's important to note that these variables contain some null and 'other' values, and the 'true' and 'false' in the legend indicates whether or not the officer had a body camera. It looks like a body camera was used mostly on victims who didn't flee followed by the victims that fled on foot. This makes sense since the majority of the victims in this dataset didn't flee.__


### Extra credit (10 points)

a. What does this code tell us? 

```{r, eval=FALSE}
mydates <- as.Date(df$date)
head(mydates)
(mydates[length(mydates)] - mydates[1])
```
__Answer: This tells us the time difference in days between the oldest and the most recent fatal shooting in this dataset.__


b. On Friday, a new report was published that was described as follows by The Guardian: "More than half of US police killings are mislabelled or not reported, study finds." Without reading this article now (due to limited time), why do you think police killings might be mislabelled or underreported?
__Answer: It might be because the ones that are recorded have missing values which causes them to be excluded from the analysis.__


c. Regarding missing values in problem 4, do you see any? If so, do you think that's all that's missing from the data?
__Answer: Yes, there were some, and I think there are more missing values in the dataset.__

# Assignment 3

**Collaborators: Elizabeth Stoner and Halle Wasser**.

This assignment is due on Canvas on Wednesday 10/27/2021 before class, at 10:15 am. Include the name of anyone with whom you collaborated at the top of the assignment.

Submit your responses as either an HTML file or a PDF file on Canvas. Also, please upload it to your website.

Save the file (found on Canvas) crime_simple.txt to the same folder as this file (your Rmd file for Assignment 3).

Load the data.
```{r}
library(readr)
library(knitr)
library(ggplot2)
library(tidyverse)
dat.crime <- read_delim("crime_simple.txt", delim = "\t")
```

This is a dataset from a textbook by Brian S. Everitt about crime in the US in 1960. The data originate from the Uniform Crime Report of the FBI and other government sources. The data for 47 states of the USA are given. 

Here is the codebook:

R: Crime rate: # of offenses reported to police per million population

Age: The number of males of age 14-24 per 1000 population

S: Indicator variable for Southern states (0 = No, 1 = Yes)

Ed: Mean of years of schooling x 10 for persons of age 25 or older

Ex0: 1960 per capita expenditure on police by state and local government

Ex1: 1959 per capita expenditure on police by state and local government

LF: Labor force participation rate per 1000 civilian urban males age 14-24

M: The number of males per 1000 females

N: State population size in hundred thousands

NW: The number of non-whites per 1000 population

U1: Unemployment rate of urban males per 1000 of age 14-24

U2: Unemployment rate of urban males per 1000 of age 35-39

W: Median value of transferable goods and assets or family income in tens of $

X: The number of families per 1000 earning below 1/2 the median income


We are interested in checking whether the reported crime rate (# of offenses reported to police per million population) and the average education (mean number of years of schooling for persons of age 25 or older) are related. 


1. How many observations are there in the dataset? To what does each observation correspond?

```{r}
nrow(dat.crime)
```

__Answer: There are 47 rows in the dataset. Each row corresponds to a US state, and the columns are various attributes of each state such as population, education, crime rate, etc.__

2. Draw a scatterplot of the two variables. Calculate the correlation between the two variables. Can you come up with an explanation for this relationship?

```{r, fig.width=6, fig.height=4}
dat.crime %>% ggplot(aes(x=Ed, y=R)) + 
  geom_point() +
  ggtitle("Scatterplot of Crime Rate vs. Education") +
  xlab("Education (Mean of years of schooling x 10 for persons of age 25 or older)") +
  ylab("Crime Rate (# of report offenses per one million)")
  
plot(dat.crime$Ed, dat.crime$R, main = "Scatterplot of Crime Rate vs. Education", 
     xlab = "Education (Mean of years of schooling x 10 for persons of age 25 or older)",
     ylab = "Crime Rate (# of report offenses per one million)")
cor(x=dat.crime$Ed, y=dat.crime$R)
```

__Answer: Based on the correlation, it looks like there is a weak positive relationship between crime rate and average education. This is somewhat expected since people with more education may be more inclined to report incidences of crime, but I'm unsure of this.__

3. Regress reported crime rate (y) on average education (x) and call this linear model `crime.lm` and write the summary of the regression by using this code, which makes it look a little nicer `{r, eval=FALSE} kable(summary(crime.lm)$coef, digits = 2)`.

```{r} 
# Remember to remove eval=FALSE above!
crime.lm <- lm(R ~ Ed, data = dat.crime)
kable(summary(crime.lm)$coef, digits = 2)
```

4. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.)

```{r} 
#residuals vs fitted plot
plot(crime.lm, which = 1) #equal variance assumption met (roughly same variance throughout given the amount of data, although subjective) and linearity assumption met (roughly horizontal line with no distinguishable pattern)
plot(crime.lm, which = 3)
#residuals vs x plot
plot(dat.crime$Ed, crime.lm$residuals, main = "Residuals vs. X", xlab = "X (Mean Years of Schooling for People 25 and Older x 10)", ylab = "Residuals") 
```

__Answer: The equal variance assumption (roughly same variance throughout given the amount of data, although subjective), the linearity assumption (roughly horizontal direction with no distinguishable pattern), and the independence assumptions are met. Even in the more sensitive approach using the scale location plot, these assumptions are somewhat met. There are some residuals that are closer together at the beginning and fan out throughout the plot, and the line is not completely horizontal. Also the errors appear independent since there is no pattern in the Residuals vs. X plot which meets the independence assumption. As mentioned before, this is subjective, but given the size of the dataset, I believe these assumptions are satisfied.__

```{r} 
plot(crime.lm, which = 5)
plot(crime.lm, which = 2)
```

__Answer: There appears to be no large residuals that have leverage in the model, but the QQ plot indicates a violation of the normality assumption since as the quantiles increase, the standardized residuals tend to curve upwards. This will affect the t tests in the model because the p-values will be too optimistic.__

5. Is the relationship between reported crime and average education statistically significant? Report the estimated coefficient of the slope, the standard error, and the p-value. What does it mean for the relationship to be statistically significant?

__Yes, it is statistically significant. The estimated coefficient is 1.12, standard error is 0.49, and its p-value is 0.03. The p-value is the probability of observing any value greater than or equal to the t value (2.29 in this case) which is how many standard deviations away the estimate is from 0. An estimate is considered to be statistically significant if the p-value is less than a certain cutoff (alpha value), usually 0.05, in which case the null hypothesis is rejected.__

6. How are reported crime and average education related? In other words, for every unit increase in average education, how does reported crime rate change (per million) per state?

__For every unit increase in Ed (mean of years of schooling x 10 for persons of age 25 or older), there is, on average, an increase in the number of reported crimes (per million per state) by 1.12.__

7. Can you conclude that if individuals were to receive more education, then crime will be reported more often? Why or why not?

__This conclusion cannot be obtained from this analysis because this regression does not model a causal relationship. There are many variables for which to control that could affect the amount of crime recorded, so we cannot say that more education causes more crimes to be reported. This analysis merely shows association between the variable, not causation. However, it must be taken into consideration that not all the assumptions were met, so one needs to be cautious about statistical inference using this model.__

# Exam 2

Data description: This dataset provides (simulated) data about 200 police departments in one year. It contains information about the funding received by the department as well as incidents of police brutality. Suppose this dataset (sim.data.csv) was collected by researchers to answer this question: **"Does having more funding in a police department lead to fewer incidents of police brutality?"**
d. Codebook:
- funds: How much funding the police department received in that year in millions of dollars.
- po.brut: How many incidents of police brutality were reported by the department that year.
- po.dept.code: Police department code

### Problem 1: EDA (10 points) 

Describe the dataset and variables. Perform exploratory data analysis for the two variables of interest: funds and po.brut.
```{r}
#load libraries
library(knitr)
library(ggplot2)
library(tidyverse)
```

```{r}
dat <- read.csv(file = 'sim.data.csv')
dim(dat)
sum(is.na(dat$po.dept.code))
sum(is.na(dat$funds))
sum(is.na(dat$po.brut))
```

__Answer: This dataset set contains 200 observations (rows), and each of them represents a police department. The columns (3 total) are attributes of each police department, and they include funds (funding received in millions of dollars for that year), po.brut (number of reported incidents of police brutality) for that year, and po.dept.code (id number for the department, most likely to protect privacy and confidentiality). There are no missing (n/a) values in this dataset.__


```{r}
summary(dat)
```
__Answer: It's interesting that in this dataset, police departments received at least $21.4 million for that year, and average is $61.04 million. It's also interesting to that that at least one department reported 0 incidences of police brutality, and the highest number of incidences reported for that year was 22. It's concerning that the average number of incidences is 18.14.__

```{r}
dat %>% ggplot(aes(x=funds)) + geom_histogram() +
                               xlab("Funds (millions of dollars)") +
                               ggtitle("Distribution of Funds")
dat %>% ggplot(aes(x=po.brut)) + geom_histogram() + 
                                 xlab("Number of Reported Incidences of Police Brutality") +
                                 ggtitle("Distribution of Police Brutality Incidences")
```
__Answer: These histograms show possible outliers in both funding and number of police brutality incidences. The distribution of funds looks to be possibly bimodal, and the distribution of police brutality looks to be slightly left skewed. These are subjective observations of the graph, but if given a larger sample, these graphs will be more representative of the population distribution of these variables.__

```{r}
dat %>% ggplot(aes(x=funds, y=po.brut)) + geom_point() + ggtitle("Scatterplot of Police Brutality vs. Funds")
cor(dat$funds, dat$po.brut)
```
__Answer: There looks to be a strong negative relationship between funds and po.brut, but it's concerning that the relationship doesn't look linear.__

### Problem 2: Linear regression (30 points)

a. Perform a simple linear regression to answer the question of interest. To do this, name your linear model "reg.output" and write the summary of the regression by using "summary(reg.output)". 

```{r}
# Remember to remove eval=FALSE!!
reg.output <- lm(po.brut ~ funds, data = dat)
summary(reg.output)
```

b. Report the estimated coefficient, standard error, and p-value of the slope. Is the relationship between funds and incidents statistically significant? Explain.

__Answer: The estimated coefficient is -0.367099, standard error is 0.004496, and the p-value is <2e-16, which is almost 0. Since the p-value is less than alpha (0.05), we are able to reject the null hypothesis that there is no association between police brutality and funds which means this relationship is statistically significant. Additionally the R-squared of the model is very high which indicates this model fits the data well.__

c. Draw a scatterplot of po.brut (y-axis) and funds (x-axis). Right below your plot command, use abline to draw the fitted regression line, like this:
```{r, fig.width=4, fig.height=4}
# Remember to remove eval=FALSE!!
plot(dat$funds, dat$po.brut, xlab = "Funds (millions of dollars)", ylab = "Number of Reported Incidences of Police Brutality", main = "Scatterplot of Police Brutality vs. Funds")
abline(reg.output, col = "red", lwd=2)
```
Does the line look like a good fit? Why or why not?

__Answer: No, it does not show a good fit because the data looks curved, and since this is a linear model, it does not fit the data properly. The R-squared is high because the linear fit does model quite a bit of the data, but it fails to capture the overall trend. If there were more observations, this might be the case. Perhaps transformation would be appropriate to apply to capture the fit of the data, but this runs the risk of overfitting. Given the size of the dataset, it would be most beneficial to split the data into test and training data to cross validate.__

d. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.) If not, what might you try to do to improve this (if you had more time)?

```{r}
#residuals vs fitted plots
plot(reg.output, which = 1)
#scale location plot
plot(reg.output, which = 3)
#residuals vs x plot
plot(dat$funds, reg.output$residuals, main = "Residuals vs. X", xlab = "X (Funding in Millions of Dollars)", ylab = "Residuals") 
```

__Answer: The linearity assumption is not met because the scatterplot of po.brut vs. funds shows a curved relationship over a linear one, and the residuals vs. x and residuals vs. fitted plots show a distinguishable curved pattern. The independence assumption is not met because there is a pattern shown in the residuals vs. x plot. The equal variance assumption is not satisfied since there are significant trends in the residuals vs. fitted and scale location plots. __

```{r}
plot(reg.output, which = 5)
plot(reg.output, which = 2)
```
__Answer: The normality assumption is violated because the QQ plot does not show a straight line like we want. Since none of the assumptions are met, I would apply a transformation to the model to see if it will help satisfy the assumptions, and to choose which transformation, I would use the Box-Cox method.__

e. Answer the question of interest based on your analysis.

__Answer: Since none of the assumptions are met, the p-value obtained by the model are optimistic which means the model can't be used for inference. If the assumptions were met, there would be an associated decrease of reported incidences of police brutality by 0.367099 when funding increases by $1 million. It is possible that it works well for prediction, but as mentioned above, the best way to make sure is to partition the data into test and train. However, with a larger sample size or with transformations, it's possible the model can be used for statistical inference.__

### Problem 3: Data ethics (10 points)

Describe the dataset. Considering our lecture on data ethics, what concerns do you have about the dataset? Once you perform your analysis to answer the question of interest using this dataset, what concerns might you have about the results?

__Answer: This dataset contains the funding in millions of dollars for the given year as well as the number of reported incidences of police brutality report by each police department. As discussed in class, the databases of police departments are prone to bias due to the biases of officers. This shows up in predictive policing where the predicted area of crime is overrepresented. Since the incidences of police brutality are reported by each department, this data may be biased and therefore produce biased results (garbage in, garbage out). This data might not completely capture the true amount of police brutality occurring since it may be underreported. It would be interesting if a separate organization is able to cross check this data with a different database of police brutality that is not recorded by police. This would hold police departments accountable in recording data and it would provide a level of fairness.__

# Assignment 4

```{r}
#install and load tidyverse library
#install.packages("tidyverse")
library(tidyverse)
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_point(mapping = aes(x = displ, y = hwy)) #plot a scatterplot of hwy vs displ
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_point(mapping = aes(x = displ, y = hwy, color = class)) #plot a scatterplot of hwy vs displ by class categories visualized as different colors
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_point(mapping = aes(x = displ, y = hwy, size = class)) #plot a scatterplot of hwy vs displ by class categories visualized as different size points
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_point(mapping = aes(x = displ, y = hwy, alpha = class)) #plot a scatterplot of hwy vs displ with class categories visualized as different shades of transparency
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_point(mapping = aes(x = displ, y = hwy, shape = class)) #plot a scatterplot of hwy vs displ with class categories visualized as different shapes
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_point(mapping = aes(x = displ, y = hwy), color = "blue") #plot a scatterplot of hwy vs displ with blue pointsx
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_point(mapping = aes(x = displ, y = hwy, color = "blue")) #plot a scatterplot of hwy vs displ with legend=blue
```

```{r}
#do not put "+" at the beginning of a line, always at the end
#ggplot(data = mpg) #call ggplot function and load data as argument
#+ geom_point(mapping = aes(x = displ, y = hwy))
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_point(mapping = aes(x = displ, y = hwy)) + #plot a scatterplot of hwy vs displ
  facet_wrap(~ class, nrow = 2) #create multiple scatterplots of hwy vs displ across different class categories
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_point(mapping = aes(x = displ, y = hwy)) + #plot a scatterplot of hwy vs displ
  facet_grid(drv ~ cyl) #create multiple scatterplots of hwy vs displ across different categories (drv and cyl)
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_smooth(mapping = aes(x = displ, y = hwy)) #plot a line of best fit of hwy vs displ
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv)) #plot a line of best fit of hwy vs displ based on drive train types
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv, color = drv), show.legend = FALSE) #plot a line of best fit of hwy vs displ based on drive train types in different colors and excluding the legend
```

```{r}
ggplot(data = mpg) + #call ggplot function and load data as argument
  geom_point(mapping = aes(x = displ, y = hwy)) + #plot a scatterplot of hwy vs displ
  geom_smooth(mapping = aes(x = displ, y = hwy)) #adding a line of best fit on top of scatterplot

```

```{r}
#another way of writing the previous function
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + #creates aesthetics of the plot
  geom_point() + #scatterplot of hwy vs displ
  geom_smooth() #line of fit over scatterplot
```

```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + #creates aesthetics of the plot
  geom_point(mapping = aes(color = class)) +#scatterplot of hwy vs displ with color dividing classes
  geom_smooth() #line of fit over scatterplot
```

```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + #creates aesthetics of the plot
  geom_point(mapping = aes(color = class)) +#scatterplot of hwy vs displ with color dividing classes
  geom_smooth(data = filter(mpg, class == "subcompact"), se = FALSE) #line of fit over scatterplot for only subcompact cars
```

```{r}
ggplot(data = diamonds) + #call ggplot function and load data as argument
  geom_bar(mapping = aes(x = cut)) #bar chart of diamond cuts
```

```{r}
ggplot(data = diamonds) + #call ggplot function and load data as argument
  stat_count(mapping = aes(x = cut)) #bar chart of diamond cuts using stat_count
```

```{r}
demo <- tribble( #create data
  ~cut,         ~freq,
  "Fair",       1610,
  "Good",       4906,
  "Very Good",  12082,
  "Premium",    13791,
  "Ideal",      21551
) 

ggplot(data = demo) + #call ggplot function and load data as argument
  geom_bar(mapping = aes(x = cut, y = freq), stat = "identity") #bar chart of diamond cuts and frequency
```

```{r}
ggplot(data = diamonds) + #call ggplot function and load data as argument
  geom_bar(mapping = aes(x = cut, y = stat(prop), group = 1)) #bar chart of diamond cuts by proportion
```

```{r}
ggplot(data = diamonds) + #call ggplot function and load data as argument
  stat_summary( #plot summary stats of each diamond cut category
    mapping = aes(x = cut, y = depth), #aesthetics of the plot
    fun.min = min, #min of each category
    fun.max = max,#max of each category
    fun = median#median of each category
  )
```

```{r}
ggplot(data = diamonds) + #call ggplot function and load data as argument
  geom_bar(mapping = aes(x = cut, colour = cut)) #bar chart of diamond cuts with bars outlined in color
ggplot(data = diamonds) + #call ggplot function and load data as argument
  geom_bar(mapping = aes(x = cut, fill = cut)) #bar chart of diamond cuts with bars filled with color
```

```{r}
ggplot(data = diamonds) + #call ggplot function and load data as argument
  geom_bar(mapping = aes(x = cut, fill = clarity)) #bar chart of diamond cuts with bar color filling for each clarity category
```

```{r}
ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + #call ggplot function and load data as argument with aesthetics
  geom_bar(alpha = 1/5, position = "identity") #barchart of cuts and divided into semitransparent clarity categories
ggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + #call ggplot function and load data as argument with aesthetics
  geom_bar(fill = NA, position = "identity") #barchart of cuts and divided into transparent clarity categories
```

```{r}
ggplot(data = diamonds) + #call ggplot function and load data as argument
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill") #bar chart of diamond cuts with all one height to compare proportions of clarity across cuts
```

```{r}
ggplot(data = diamonds) + #call ggplot function and load data as argument
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge") #bar chart of diamond cuts with distribution of clarity for each cut
```

```{r}
ggplot(data = mpg) +  #call ggplot function and load data as argument
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter") #scatterplot of hwy vs displ with points jittered randomly
```

```{r}
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + #call ggplot function and load data as argument and sets aesthetics
  geom_boxplot() #boxplot of class and hwy
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + #call ggplot function and load data as argument and sets aesthetics
  geom_boxplot() + #boxplot of class and hwy
  coord_flip() #flips x and y of plot
```
```{r}
nz <- map_data("nz") #create dataframe

ggplot(nz, aes(long, lat, group = group)) + #call ggplot with data and set aesthetics
  geom_polygon(fill = "white", colour = "black") #create map with white fill and black outline

ggplot(nz, aes(long, lat, group = group)) + #call ggplot with data and set aesthetics
  geom_polygon(fill = "white", colour = "black") + #create map with white fill and black outline
  coord_quickmap() #sets aspect ratio
```

```{r}
bar <- ggplot(data = diamonds) + #creates bar, calls ggplot and loads data
  geom_bar( #bar chart
    mapping = aes(x = cut, fill = cut), #barchart of cut
    show.legend = FALSE, #don't show legend
    width = 1
  ) + 
  theme(aspect.ratio = 1) + #color graph with theme
  labs(x = NULL, y = NULL) #null x and y labels

bar + coord_flip() #flip x and y
bar + coord_polar() #use polar coordinates
```

```{r}
ggplot(mpg, aes(displ, hwy)) + #call ggplot, load data, and create aesthetics
  geom_point(aes(color = class)) + #scatterplot of hwy vs displ with color = class
  geom_smooth(se = FALSE) + #line of fit without standard error
  labs(title = "Fuel efficiency generally decreases with engine size") #add title
```

```{r}
ggplot(mpg, aes(displ, hwy)) + #call ggplot, load data, and create aesthetics
  geom_point(aes(color = class)) + #scatterplot of hwy vs displ with color = class
  geom_smooth(se = FALSE) + #line of fit without standard error
  labs(
    title = "Fuel efficiency generally decreases with engine size",
    subtitle = "Two seaters (sports cars) are an exception because of their light weight",
    caption = "Data from fueleconomy.gov"
  ) #add title, subtitle, and caption
```

```{r}
ggplot(mpg, aes(displ, hwy)) + #call ggplot, load data, and create aesthetics
  geom_point(aes(color = class)) + #scatterplot of hwy vs displ with color = class
  geom_smooth(se = FALSE) + #line of fit without standard error
  labs(
    x = "Engine displacement (L)",
    y = "Highway fuel economy (mpg)",
    colour = "Car type"
  ) #add x, y, and legend labels
```

```{r}
df <- tibble(
  x = runif(10),
  y = runif(10)
) #create data
ggplot(df, aes(x, y)) + #ggplot, load data
  geom_point() + #scatterplot of y vs x
  labs(
    x = quote(sum(x[i] ^ 2, i == 1, n)),
    y = quote(alpha + beta + frac(delta, theta))
  ) #create labels for mathmetical expressions for x and y
```

```{r}
best_in_class <- mpg %>% #create best_in_class data
  group_by(class) %>%
  filter(row_number(desc(hwy)) == 1) #filter

ggplot(mpg, aes(displ, hwy)) + #ggplot, load data, and set aesthetics
  geom_point(aes(colour = class)) + #scatterplot of hwy vs displ with color = class
  geom_text(aes(label = model), data = best_in_class) #create labels for points in best_in_class
```

```{r}
ggplot(mpg, aes(displ, hwy)) + #ggplot, load data, and set aesthetics
  geom_point(aes(colour = class)) + #scatterplot of hwy vs displ with color = class
  geom_label(aes(label = model), data = best_in_class, nudge_y = 2, alpha = 0.5) #create labels for points in best_in_class and adjust so they're not overlapping
```

```{r}
ggplot(mpg, aes(displ, hwy)) + #ggplot, load data, and set aesthetics
  geom_point(aes(colour = class)) + #scatterplot of hwy vs displ with color = class
  geom_point(size = 3, shape = 1, data = best_in_class) + #adjust size and data
  ggrepel::geom_label_repel(aes(label = model), data = best_in_class) #create labels for points in best_in_class and adjust so they're not overlapping
```

```{r}
class_avg <- mpg %>% #create class_avg dataframe
  group_by(class) %>%
  summarise(
    displ = median(displ),
    hwy = median(hwy)
  )
#> `summarise()` ungrouping output (override with `.groups` argument)

ggplot(mpg, aes(displ, hwy, colour = class)) + #ggplot, data load, and set aesthetics
  ggrepel::geom_label_repel(aes(label = class), #create labels
    data = class_avg,
    size = 6,
    label.size = 0,
    segment.color = NA
  ) +
  geom_point() + #scatterplot of hwy vs displ
  theme(legend.position = "none") #color with theme
```

```{r}
label <- mpg %>% #create label dataframe
  summarise(
    displ = max(displ),
    hwy = max(hwy),
    label = "Increasing engine size is \nrelated to decreasing fuel economy."
  )

ggplot(mpg, aes(displ, hwy)) + #ggplot, data load, and set aesthetics
  geom_point() + #scatterplot of hwy vs displ
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right") #create text label
```

```{r}
label <- tibble( #create label dataframe
  displ = Inf,
  hwy = Inf,
  label = "Increasing engine size is \nrelated to decreasing fuel economy."
)

ggplot(mpg, aes(displ, hwy)) + #ggplot, data load, and set aesthetics
  geom_point() + #scatterplot of hwy vs displ
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right") #create text labels on the border
```

```{r} 
#add line breaks given the number of characters chosen per line
"Increasing engine size is related to decreasing fuel economy." %>%
  stringr::str_wrap(width = 40) %>%
  writeLines()
#> Increasing engine size is related to
#> decreasing fuel economy.
```

```{r}
ggplot(mpg, aes(displ, hwy)) + #ggplot, data load, and set aesthetics
  geom_point(aes(colour = class)) #scatterplot of hwy vs displ with class color differentiation

#default scales of the above plot (behind the scenes)
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  scale_x_continuous() +
  scale_y_continuous() +
  scale_colour_discrete()
```

```{r}
ggplot(mpg, aes(displ, hwy)) + #ggplot, data load, and set aesthetics
  geom_point() + #scatterplot of hwy vs displ
  scale_y_continuous(breaks = seq(15, 40, by = 5)) #controls position of ticks in plot
```

```{r}
ggplot(mpg, aes(displ, hwy)) + #ggplot, data load, and set aesthetics
  geom_point() + #scatterplot of hwy vs displ
  scale_x_continuous(labels = NULL) + #no labels for ticks on x axis
  scale_y_continuous(labels = NULL) #no labels for ticks on y axis
```

```{r}
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id)) +
    geom_point() + #scatterplot of id vs start
    geom_segment(aes(xend = end, yend = id)) + #scale of plot
    scale_x_date(NULL, breaks = presidential$start, date_labels = "'%y") #controls ticks and labels of x axis
```

```{r}
base <- ggplot(mpg, aes(displ, hwy)) +  #ggplot, data load, and set aesthetics
  geom_point(aes(colour = class)) #scatterplot of hwy vs displ with class differentiation

base + theme(legend.position = "left") #legend on left
base + theme(legend.position = "top") #legend on top
base + theme(legend.position = "bottom") #legend on bottom
base + theme(legend.position = "right") # the default
```

```{r}
ggplot(mpg, aes(displ, hwy)) + #ggplot, data load, and set aesthetics
  geom_point(aes(colour = class)) + #scatterplot of hwy vs displ with class differentiation
  geom_smooth(se = FALSE) + #line of fit without standard error
  theme(legend.position = "bottom") + #position of legend at bottom
  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 4))) #layout of legend in 1 row
#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'
```

```{r}
ggplot(diamonds, aes(carat, price)) + #ggplot, data load, and set aesthetics
  geom_bin2d() #heatmap of 2d bins

ggplot(diamonds, aes(log10(carat), log10(price))) + #ggplot, data load, and log of aesthetics
  geom_bin2d() #heatmap of 2d bins
```

```{r}
ggplot(diamonds, aes(carat, price)) + #ggplot, data load, set aesthetics
  geom_bin2d() + #heatmap of 2d bins
  scale_x_log10() + #log10 of x scale
  scale_y_log10() #log10 of y scale
```

```{r}
ggplot(mpg, aes(displ, hwy)) + #ggplot, data load, set aesthetics
  geom_point(aes(color = drv)) #scatterplot of hwy vs displ with drv color category differentiation

ggplot(mpg, aes(displ, hwy)) + #ggplot, data load, set aesthetics
  geom_point(aes(color = drv)) + #scatterplot of hwy vs displ with drv color category differentiation
  scale_colour_brewer(palette = "Set1") #scale by color
```

```{r}
ggplot(mpg, aes(displ, hwy)) + #ggplot, data load, set aesthetics
  geom_point(aes(color = drv, shape = drv)) + #scatterplot of hwy vs displ with drv color category and shape differentiation
  scale_colour_brewer(palette = "Set1") #scale by color
```

```{r}
presidential %>%
  mutate(id = 33 + row_number()) %>% #select columns and rows
  ggplot(aes(start, id, colour = party)) + #ggplot, data load, set aesthetics
    geom_point() + #scatterplot
    geom_segment(aes(xend = end, yend = id)) + #segment plot
    scale_colour_manual(values = c(Republican = "red", Democratic = "blue")) #color scale for certain points
```

```{r}
df <- tibble( #create data
  x = rnorm(10000),
  y = rnorm(10000)
)
ggplot(df, aes(x, y)) + #ggplot, data load, set aesthetics
  geom_hex() + #create hexagonal shape
  coord_fixed() #fixed aspect ratio

ggplot(df, aes(x, y)) + #ggplot, data load, set aesthetics
  geom_hex() + #create hexagonal shape
  #viridis::scale_fill_viridis() + #contrasting colors in shape
  coord_fixed() #fixed aspect ratio
```

```{r}
ggplot(mpg, mapping = aes(displ, hwy)) + #ggplot, data load, set aesthetics
  geom_point(aes(color = class)) + #scatterplot of hwy vs displ with class color category differentiation
  geom_smooth() + #line of fit on top of scatterplot
  coord_cartesian(xlim = c(5, 7), ylim = c(10, 30)) #adjust x and y limits

mpg %>%
  filter(displ >= 5, displ <= 7, hwy >= 10, hwy <= 30) %>% #filter rows
  ggplot(aes(displ, hwy)) + #ggplot, data load, set aesthetics
  geom_point(aes(color = class)) + #scatterplot of hwy vs displ with class color category differentiation
  geom_smooth() #line of fit on top of scatterplot
```

```{r}
suv <- mpg %>% filter(class == "suv") #create suv dataframe
compact <- mpg %>% filter(class == "compact") #create compact dataframe

ggplot(suv, aes(displ, hwy, colour = drv)) + #ggplot, data load, set aesthetics
  geom_point() #scatterplot of hwy vs displ with drv color category differentiation

ggplot(compact, aes(displ, hwy, colour = drv)) + #ggplot, data load, set aesthetics
  geom_point() #scatterplot of hwy vs displ with drv color category differentiation
```

```{r}
x_scale <- scale_x_continuous(limits = range(mpg$displ)) #create x_scale dataframe
y_scale <- scale_y_continuous(limits = range(mpg$hwy)) #create y_scale dataframe
col_scale <- scale_colour_discrete(limits = unique(mpg$drv)) #create col_scale dataframe

ggplot(suv, aes(displ, hwy, colour = drv)) + #ggplot, data load, set aesthetics
  geom_point() + #scatterplot of hwy vs displ with drv color category differentiation
  x_scale + #adjust x scale
  y_scale + #adjust y scale
  col_scale #adjust color scale

ggplot(compact, aes(displ, hwy, colour = drv)) + #ggplot, data load, set aesthetics
  geom_point() + #scatterplot of hwy vs displ with drv color category differentiation
  x_scale + #adjust x scale
  y_scale + #adjust y scale
  col_scale #adjust color scale
```

```{r}
ggplot(mpg, aes(displ, hwy)) +  #ggplot, data load, set aesthetics
  geom_point(aes(color = class)) + #scatterplot of hwy vs displ with class color category differentiation
  geom_smooth(se = FALSE) + #line of best fit without standard error
  theme_bw() #adjust theme
```
```{r}
ggplot(mpg, aes(displ, hwy)) + geom_point() #create scatterplot of hwy vs displ

ggsave("my-plot.pdf") #save plot
```

# Final Project

**Collaborators: Elizabeth Stoner, Natalie Yang, Sophia Restaino, and Johanna Doherty**

```{r}
library(plyr)
releases <- read.csv("pdp_state_releases.csv")
untouched.releases <- releases
```

### Introduction

For our final project, we looked at how different demographic factors are associated with prison release. We used a dataset from Open Data Philly, which includes information about the people released back into society coming out of both the Philadelphia Department of Prisons and the Pennsylvania Department of Corrections. This data shows a snapshot of release history, with data from 2015. Overall, it looks at the people who were charged with a criminal non-summary type offense[1].
The data contains information on 24,089 released individuals. The six columns of data include the custody agency from which each person was released, the date of their release, and some demographic information regarding sex, age, and race or ethnicity. In addition, it includes zip code data for each person. There are no nulls in the data except for zip code, but this is not a variable of interest. Although we will look at each of these demographic factors alongside race in our data analysis, our main research question for the project is: Does race have an effect on which inmates are released back into Philadelphia from State or Philadelphia prisons?
For some motivation and context into the chosen research question, it is important to understand the difference between the city and state prison systems. The PA State Prison System releases inmates back into Philadelphia from just prisons, whereas the Philadelphia Department of Prisons includes jails. This is relevant because where prisons house inmates already convicted of more serious crimes, jails house both people convicted of minor crimes as well as those awaiting trial who have not yet been convicted. We decided that this distinction would make it interesting to see how race may be associated with whether an individual is released back to Philly from the PDP vs. the PA State system.

### Exploratory Data Analysis

The following exploratory data analysis will examine the main variable of interest, race, as well as each of the other demographic factors with number of releases in order to get a preliminary understanding of the relationship between the variables.

**Race and Releases**
This barplot shows the distribution of race/ethnicity of people released. The highest number are Black followed by white and Hispanic. This is likely due in part to the disparities in incarceration rates of people of color as compared to whites, but the large differences between these frequencies calls for further exploration of this variable as it relates to releases. This could also be due to the racial demographics of Philadelphia.

```{r}
barplot(table(releases$race_ethnicity),
        main = 'Barplot of Race/Ethnicity',
        xlab = 'Race/Ethnicity',
        ylab = 'Frequency')
```

**Age and Releases**
In the below histogram we see that the data distribution is skewed to the right, with the most common age range of those released being between the ages of 20 and 40. This is likely due to the fact that this is also the most common age for crime and offending in general, and there is likely a higher proportion of inmates between 20 and 40 overall.

```{r}
hist(releases$age,
     main = "Histogram of Ages",
     xlab = "Age",
     ylab = "Frequency",
     ylim = c(0, 5000),
     xlim = c(0,100))
```

**Sex and Releases**
In the below histogram we see that there were far more male inmates released than female. This is likely due to the fact that there are higher populations of male inmates in Philadelphia prisons (and prisons in general) overall, so there are more of them to be released.

```{r}
barplot(table(releases$sex),
     main = "Barplot of Sex",
     ylab = "Frequency",
     xlab = "Sex")
```

**Custody Agency and Releases**
In the below histogram, demonstrating the frequency of release by Custody Agency (0 - 0.5 representing the Phila Dept of Prisons and 0.5 - 1 representing the State Prison System), we can see that the Philadelphia Department of Prisons released many more inmates in 2015 than did the state system.

```{r}
releases$custody_numeric <- 0
releases$custody_numeric[releases$custody_agency == "State"] <- 1

barplot(table(releases$custody_agency),
     main = "Barplot of Custody Agencies",
     ylab = "Frequency",
     xlab = "Custody Agency")
```

### Linear Regression

For the analysis of this data set, we chose to do a linear regression as our model. We transformed the custody_agency variable to be binary in which 1 = PDP and 0 = State, and we performed a transformation of the race_ethnicity variable to be binary in which 0 = White and 1 = Black, Asian, Hispanic, and Other. We regressed the custody agency (binary) on race/ethnicity (binary) to look at the relationship between these variables. In the sections below we will address the assumptions to perform a linear regression, look at the results of the regression, and discuss the results and conclusions of the regression analysis.

```{r}
releases$race_eth_binary <- revalue(releases$race_ethnicity,c("White" = 0,
                                                               "Asian" = 1,
                                                               "Hispanic" = 1,
                                                               "Other" = 1,
                                                               "Black" = 1))
ols.race <- lm(custody_numeric ~ race_eth_binary, data = releases)
summary(ols.race)
```

### Assumptions

```{r}
plot(ols.race, which=1)
plot(ols.race, which=3)
plot(ols.race, which=5)
plot(ols.race)
```

**Linearity**
Looking at the below residuals vs. fitted plot, we can see that the scatterplot smoother, showing the average value of the residuals at each value of fitted value, is pretty much flat across the plot. However, there are a group of outlying data points at the top right of the graph. The flatness of the red line indicates that there is no discernable non-linear pattern to the data. While the group of outlier data points is concerning, we will cautiously decide to proceed as though the linearity assumption is satisfied.

**Independence**
While the independence assumption cannot be proved as true, we can see from the Residuals vs. Fitted plot that there does not seem to be any discernible pattern of the data, suggesting that the variables are independent of each other. We must note the outlying points in the top right corner of the plot, but it is unclear at this point in our analysis how influential they may be. Therefore, we will also cautiously consider the independence assumption satisfied.

**Homoscedasticity**
In the Scale-Location plot shown below, there is a clear trend to the red line on this plot, and there is a significant group of data points that do not fit the trend at the top right corner. This plot indicates that the residuals have non-constant variance. Therefore, the homoscedasticity assumption, that all the errors have the same variance, is not satisfied by this data.

**Normality**
As seen in the below normal Q-Qplot, the data really do not follow the trend of the line. Not only are they grouped in certain areas, but they also extend far past the line, to both the left and right, in each different grouping of data points. This demonstrates to us that the data is not normally distributed and does not satisfy the normality assumption. 

### Regression Results / Causal Analysis

For the regression of the custody numeric onto race, the results are as follows; the estimated coefficient is .0052, the standard error is .0069, and the p-value of the slope is .4537. Using 0.05 as our confidence interval, the p-value of .4537 shows us that the relationship between race and number of releases is not statistically significant. This p-value essentially tells us that if the null hypothesis (no relationship between the two variables) were true, the chances of observing a statistic such as this data set would be 45.37%, which is a pretty high chance. Given the results of this linear regression, it seems that there is no statistically significant relationship between the number of releases and the race of the inmates being released from Philadelphia prisons.
Although this is a very interesting topic to examine, it is not currently, and would not be able to be turned into, a casual estimate. Any relationship between an inmates release and their race would be solely correlational. While its possible for race to play a factor, even if subconsciously, in the prison systems decision to release certain inmates, someones race itself is not causing their release or lack of release from prison. The purpose of this data analysis was to determine whether there was a relationship between the inmates released from prison in Philadelphia and their race, but there was no causal question to be addressed in this research. There is no way to make this research into a causal analysis, and the question at hand is merely correlational.

### Causal DAG

![](/Users/carmenavery/Documents/Penn/Senior/Fall/CRIM 250/GitHub Website/CarmenAvery/images/DAG.png)
### Discussion and Ideas for Future Work

Based on the linear regression analysis which regressed race onto Custody Agency data, there does not appear to be a statistically significant relationship between race and those released back into Philadelphia from city vs. state prisons/jails. However, there are a few factors that make us unsure of the reliability of this conclusion. Firstly, this data set did not meet all of the assumptions for performing a linear regression analysis. The Normality, Homoscedasticity, and Independence assumptions were not met, and the Linearity assumption was not solidly met. This is likely due to the fact that both variables were coded as binary, and a logistic regression would probably have been a better fit for this dataset and question. Secondly, even if the assumptions had been met to perform a linear regression for this data, any results found may not tell the whole story. Given that the demographics of Philadelphia prisons include higher proportions of people of color (11.8% White vs. 88.2% POC in 2015),[2] if an accurately completed regression had indicated that there was a positive association between being released from the State or PDP and being a person of color, this may just come down to there being more people of color in prisons to be released. Given these demographics, the result that would be interesting would be a positive association between being released and being white, or even no association at all, which would indicate potential discrimination in the release process. If one were to complete future research using this data set and transforming the data to perform linear regression, these are all factors that would have to be taken into consideration.
Despite not finding a causal relationship in our analysis, identifying bias and other problems in the prison system, especially regarding releases, is still worthwhile. Future research could include an examination of early releases versus releases of prisoners at the end of their sentence. Such an analysis could also include studying the behavior of inmates during their incarceration and how they are treated by prison staff and fellow inmates, factors which all work together to affect the possibility of early release. Additionally, since eligibility for parole is determined by human decision making, it is a process during which there can unfortunately be racial bias. A study of this issue could analyze racial disparities between inmates who are granted parole and those who have applied but are denied parole. This future research would also have to take the higher proportions of POC inmates into account when completing the statistical analysis. Another associated topic is sentencing. Research of bias in this department could address sentencing length disparities between people who committed the same type of crime but are of different races.

[1] People released to Philadelphia from prison & jail. OpenDataPhilly. (2018, March 8). Retrieved November 21, 2021, from https://www.opendataphilly.org/dataset/prison-releases.
[2] Philadelphia Jail Population Report | July 2015  April 2019. Research and Development First Judicial District of Pennsylvania. Phila Gov. Accessed December 3, 2021. https://www.phila.gov/media/20190513144034/April-2019-Full-Jail-Report.pdf.
